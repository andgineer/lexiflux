{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4594a37f-3aaf-459b-b31c-502a4f44ac78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key is valid. Available models: ['whisper-1', 'dall-e-2', 'gpt-3.5-turbo-16k', 'tts-1-hd-1106', 'tts-1-hd', 'gpt-4-turbo-2024-04-09', 'gpt-4-0125-preview', 'gpt-4-turbo-preview', 'gpt-4-turbo', 'gpt-3.5-turbo-instruct-0914', 'gpt-3.5-turbo', 'gpt-3.5-turbo-instruct', 'text-embedding-3-small', 'tts-1', 'text-embedding-3-large', 'gpt-4-1106-preview', 'babbage-002', 'gpt-4o', 'gpt-3.5-turbo-0125', 'gpt-4o-2024-05-13', 'tts-1-1106', 'dall-e-3', 'gpt-4-0613', 'text-embedding-ada-002', 'gpt-4', 'davinci-002', 'gpt-3.5-turbo-1106']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "import openai\n",
    "\n",
    "# For ChatGPT-4\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# alternatively set LANGCHAIN_API_KEY\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"\"  # \"true\"\n",
    "\n",
    "try:\n",
    "    models = openai.models.list()\n",
    "    print(\"API key is valid. Available models:\", [model.id for model in models.data])\n",
    "except openai.error.AuthenticationError as e:\n",
    "    print(\"API key is invalid:\", str(e))\n",
    "\n",
    "openai_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", api_key=os.getenv(\"OPENAI_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db89eff5-e02d-467c-9f37-8906da6bf007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama3_request(prompt):\n",
    "    # This function should interact with your local Llama3 model.\n",
    "    # Replace the below code with actual API or command-line interaction.\n",
    "    import subprocess\n",
    "    result = subprocess.run(['ollama', 'run', 'llama3', '--prompt', prompt], capture_output=True, text=True)\n",
    "    return result.stdout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ef5bf677-c750-40a3-a6ac-d3495f7abf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI()\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Prompt templates\n",
    "translation_system_template = \"\"\"Given the following text in {source_language}, \n",
    "translate the word marked as <b>{word}</b> from {source_language} to {target_language}. Put into result only word translation without surraunding <b></b>\"\"\"\n",
    "\n",
    "translation_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    ('system', translation_system_template),\n",
    "    ('user', '{text}')\n",
    "])\n",
    "translation_chain = translation_prompt_template | model | parser\n",
    "\n",
    "\n",
    "examples_system_template = \"\"\"Given the following text in {source_language}, \n",
    "provide three examples in {source_language} of sentences using the word marked as <b>{word}</b> in his meaning in the context.\"\"\"\n",
    "\n",
    "examples_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    ('system', examples_system_template),\n",
    "    ('user', '{text}')\n",
    "])\n",
    "examples_chain = examples_prompt_template | model | parser\n",
    "\n",
    "\n",
    "explanation_system_template = \"\"\"Given the following text in {source_language}, \n",
    "using only {source_language} explain the meaning of the word marked as <b>{word}</b>.\"\"\"\n",
    "\n",
    "explanation_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    ('system', explanation_system_template),\n",
    "    ('user', '{text}')\n",
    "])\n",
    "explanation_chain = explanation_prompt_template | model | parser\n",
    "\n",
    "\n",
    "# Function to interact with Llama3\n",
    "def llama3_chain(prompt_template, **kwargs):\n",
    "    prompt = prompt_template.format(**kwargs)\n",
    "    response = llama3_request(prompt)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0f95c324-b66c-4936-acda-7cecd7004623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_translation_examples_and_explanation(text: str, word: str, source_language: str, target_language: str, model: str = 'gpt'):\n",
    "    # Mark the specific word in the text\n",
    "    marked_text = text.replace(word, f\"<b>{word}</b>\", 1)\n",
    "    if model == 'gpt':\n",
    "        translation = translation_chain.invoke({\n",
    "            \"text\": marked_text, \"word\": word, \"source_language\": source_language, \"target_language\": target_language})\n",
    "        examples = examples_chain.invoke({\n",
    "            \"text\": marked_text, \"word\": word, \"source_language\": source_language, \"target_language\": target_language})\n",
    "        explanation = explanation_chain.invoke({\n",
    "            \"text\": marked_text, \"word\": word, \"source_language\": source_language, \"target_language\": target_language})\n",
    "    elif model == 'llama3':\n",
    "        translation = llama3_chain(translation_prompt_template, text=marked_text, word=word, source_language=source_language, target_language=target_language)\n",
    "        # examples = llama3_chain(example_prompt_template, text=marked_text, word=word, source_language=source_language, target_language=target_language)\n",
    "        # explanation = llama3_chain(explanation_prompt_template, text=marked_text, word=word, source_language=source_language)\n",
    "    return translation, examples, explanation\n",
    "\n",
    "def format_results_in_html(translation: str, examples: str, explanation: str) -> str:\n",
    "    html_content = f\"\"\"\n",
    "    <div>\n",
    "        <h2>Translation</h2>\n",
    "        <p>{translation}</p>\n",
    "        <h2>Usage Examples</h2>\n",
    "        <ul>\n",
    "    \"\"\"\n",
    "    for example in examples.split(\"\\n\"):\n",
    "        html_content += f\"<li>{example}</li>\"\n",
    "    \n",
    "    html_content += f\"\"\"\n",
    "        </ul>\n",
    "        <h2>Explanation</h2>\n",
    "        <p>{explanation}</p>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    return f\"{translation}\\n{examples}\\n{explanation}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "38b2232f-0272-4621-bcc5-f9ffe75cc4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPT-4 Results:\n",
      "Love\n",
      "1. Duboko verujem u snagu <b>ljubavi</b> koja nas spaja i čini da sve probleme prevaziđemo zajedno.\n",
      "2. Njihova veza je puna strasti i <b>ljubavi</b>, moguće je osetiti toplinu njihovih osmeha dok su zajedno.\n",
      "3. <b>Ljubav</b> prema prirodi je ono što me inspiriše da očuvam ovaj predivan svet za buduće generacije.\n",
      "<b>Ljubav</b> predstavlja snažan osećaj topline, privrženosti i pažnje prema nekome ili nečemu. To je osećaj koji nas pokreće, čini da se osećamo srećno i ispunjeno.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "serbian_text = \"Ljubav je najlepša stvar na svetu. Ljubav pokreće sve.\"\n",
    "word_to_translate = \"Ljubav\"\n",
    "source_language = \"Serbian\"\n",
    "target_language = \"English\"\n",
    "\n",
    "# Test with ChatGPT-4\n",
    "translation_gpt, examples_gpt, explanation_gpt = get_translation_examples_and_explanation(serbian_text, word_to_translate, source_language, target_language, model='gpt')\n",
    "\n",
    "# Test with Llama3\n",
    "# translation_llama, examples_llama, explanation_llama = get_translation_examples_and_explanation(serbian_text, word_to_translate, source_language, target_language, model='llama3')\n",
    "\n",
    "# Display results in HTML\n",
    "html_result_gpt = format_results_in_html(translation_gpt, examples_gpt, explanation_gpt)\n",
    "# html_result_llama = format_results_in_html(translation_llama, examples_llama, explanation_llama)\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "print(\"ChatGPT-4 Results:\")\n",
    "# display(HTML(html_result_gpt))\n",
    "print(html_result_gpt)\n",
    "\n",
    "# print(\"Llama3 Results:\")\n",
    "# display(HTML(html_result_llama))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcd8a80-9930-4157-bd89-c840c7007e89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
